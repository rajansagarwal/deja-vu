{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence_transformers umap umap-learn langchain cohere faiss textract moviepy google-cloud-speech pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python-headless ipywidgets pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!sudo apt update && sudo apt install ffmpeg\n",
    "!pip install --upgrade moviepy\n",
    "!pip install librosa\n",
    "\n",
    "import whisper\n",
    "import time\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_model = whisper.load_model(\"base.en\")\n",
    "small_model = whisper.load_model(\"small.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "input_folder = \"files/\"\n",
    "output_folder = \"fragments/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def split_video_into_fragments(input_video_file, fragment_duration):\n",
    "    fragment_num = 61\n",
    "\n",
    "    clip = VideoFileClip(input_video_file)\n",
    "    duration = int(clip.duration)\n",
    "\n",
    "    fragment_start = 0\n",
    "    fragment_end = fragment_duration\n",
    "\n",
    "    while fragment_end <= duration:\n",
    "        output_file = os.path.join(output_folder, f\"fragment_{fragment_num}.mov\")\n",
    "        ffmpeg_extract_subclip(input_video_file, fragment_start, fragment_end, targetname=output_file)\n",
    "\n",
    "        print(f\"Processed fragment {fragment_num}\")\n",
    "\n",
    "        fragment_start += fragment_duration\n",
    "        fragment_end += fragment_duration\n",
    "        fragment_num += 1\n",
    "\n",
    "    clip.close()\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".MOV\"):\n",
    "        input_video_file = os.path.join(input_folder, filename)\n",
    "        split_video_into_fragments(input_video_file, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = []\n",
    "file_names = []\n",
    "\n",
    "for i in range(61, 66):\n",
    "    video_file = f'fragment_{i}.mov'\n",
    "    video_path = \"fragments/\" + video_file\n",
    "    audio_path = \"audio/\" + video_file[:-4] + \".wav\"\n",
    "\n",
    "    y, sr = librosa.load(video_path, sr=16000)\n",
    "    sf.write(audio_path, y, sr)\n",
    "\n",
    "    result = small_model.transcribe(audio_path)\n",
    "    text = result[\"text\"].strip()\n",
    "\n",
    "    text_file = video_file[:-4] + \".txt\"\n",
    "    text_path = \"video_fragments/\" + text_file\n",
    "    transcriptions.append(text)\n",
    "    file_names.append(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install umap-learn opencv-python Pillow\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "interval = 10\n",
    "output_directory = \"frame_images\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "clip_results = []\n",
    "\n",
    "for i in range(61, 66):\n",
    "    video_file = f'fragment_{i}.mov'\n",
    "    video_path = \"fragments/\" + video_file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames_to_skip = fps * interval\n",
    "    frame_count = 0\n",
    "    texts = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frames_to_skip == 0:\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            captions = captioner(image)\n",
    "            if captions:\n",
    "                texts.append(captions[0]['generated_text'])\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    long_sentence = ' '.join(texts)\n",
    "\n",
    "    clip_results.append(long_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'File Name': file_names, 'Transcription': transcriptions, 'Context': clip_results })\n",
    "df.to_csv('df2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
